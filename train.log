ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2026.1.4: Fast Gemma3 patching. Transformers: 4.57.6.
   \\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.552 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.10.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.6.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.34. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Gemma3 does not support SDPA - switching to fast eager.
Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients
Map:   0%|          | 0/52800 [00:00<?, ? examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19000/52800 [00:00<00:00, 165145.82 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38000/52800 [00:00<00:00, 158797.53 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52800/52800 [00:00<00:00, 166942.88 examples/s]
Map:   0%|          | 0/880 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 880/880 [00:00<00:00, 160018.53 examples/s]
Unsloth: Tokenizing ["text"] (num_proc=20):   0%|          | 0/52800 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):   2%|â–         | 1000/52800 [00:02<02:02, 422.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):   4%|â–         | 2000/52800 [00:02<01:04, 792.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):   9%|â–‰         | 4640/52800 [00:03<00:26, 1826.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  11%|â–ˆ         | 5640/52800 [00:03<00:24, 1904.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  19%|â–ˆâ–‰        | 9920/52800 [00:04<00:11, 3600.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  24%|â–ˆâ–ˆâ–       | 12560/52800 [00:04<00:10, 4015.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  29%|â–ˆâ–ˆâ–‰       | 15200/52800 [00:05<00:08, 4357.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18840/52800 [00:05<00:06, 5166.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20480/52800 [00:06<00:06, 4640.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24120/52800 [00:07<00:05, 5360.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25760/52800 [00:07<00:05, 4766.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 29400/52800 [00:08<00:04, 5421.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32040/52800 [00:08<00:03, 5331.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33680/52800 [00:09<00:04, 4662.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 37320/52800 [00:09<00:02, 5262.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38960/52800 [00:10<00:02, 4670.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 42600/52800 [00:10<00:01, 6868.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44240/52800 [00:10<00:01, 6198.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 45240/52800 [00:10<00:01, 6538.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 46880/52800 [00:11<00:01, 5758.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48520/52800 [00:11<00:00, 6087.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49520/52800 [00:11<00:00, 5918.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 51160/52800 [00:11<00:00, 6245.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 52160/52800 [00:11<00:00, 6143.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52800/52800 [00:12<00:00, 4231.36 examples/s]
Unsloth: Tokenizing ["text"] (num_proc=20):   0%|          | 0/880 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):   5%|â–Œ         | 44/880 [00:01<00:36, 22.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  10%|â–ˆ         | 88/880 [00:02<00:18, 41.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  15%|â–ˆâ–Œ        | 132/880 [00:02<00:13, 56.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  20%|â–ˆâ–ˆ        | 176/880 [00:03<00:10, 68.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  25%|â–ˆâ–ˆâ–Œ       | 220/880 [00:03<00:08, 78.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  30%|â–ˆâ–ˆâ–ˆ       | 264/880 [00:04<00:07, 84.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 308/880 [00:04<00:06, 89.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 352/880 [00:04<00:05, 92.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 396/880 [00:05<00:05, 94.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 440/880 [00:05<00:04, 92.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 484/880 [00:06<00:04, 92.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 528/880 [00:06<00:03, 95.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 572/880 [00:07<00:03, 97.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 616/880 [00:07<00:02, 98.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 660/880 [00:08<00:02, 99.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 704/880 [00:08<00:01, 99.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 748/880 [00:09<00:01, 100.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 792/880 [00:09<00:00, 113.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 836/880 [00:09<00:00, 112.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 880/880 [00:10<00:00, 115.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=20): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 880/880 [00:10<00:00, 85.61 examples/s] 
The model is already on multiple devices. Skipping the move to device specified in `args`.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 52,800 | Num Epochs = 1 | Total steps = 500
O^O/ \_/ \    Batch size per device = 4 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16
 "-____-"     Trainable parameters = 65,576,960 of 4,365,656,432 (1.50% trained)
Starting training...
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:11<1:39:44, 11.99s/it]  0%|          | 2/500 [00:14<52:19,  6.30s/it]    1%|          | 3/500 [00:16<37:08,  4.48s/it]  1%|          | 4/500 [00:18<30:01,  3.63s/it]  1%|          | 5/500 [00:21<26:04,  3.16s/it]                                                 1%|          | 5/500 [00:21<26:04,  3.16s/it]  1%|          | 6/500 [00:23<23:38,  2.87s/it]  1%|â–         | 7/500 [00:25<22:04,  2.69s/it]